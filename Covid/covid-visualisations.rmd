---
title: "Covid Visualisations"
author: "Paul Williams"
date: '2022-04-05'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Learning to Visualise in R using Covid data

I've started this project so I can learn more about visualizing data in R using
ggplot, ggmap, and other Grammar of Graphics based tools.

## Data source

Data download link: https://covid.ourworldindata.org/data/owid-covid-data.csv

The data used has been sourced from https://ourworldindata.org. A download l1ink
and list of their data sources is available via the README.md of this Github
project https://github.com/owid/covid-19-data/tree/master/public/data. The
README.md also lists the metadata, i.e. descriptions of variables and other
links for understanding the data and how it was collected.

The visualizations and insights presented or mentioned in this project are only
as reliable as the underlying data and my relatively apprentice level ability.

## Dependencies

I currently prefer to specify my dependencies upfront so they're installed
and ready to go. As I'm fairly new to the field my best strategy is to follow
industry norms, practices, and tools such as `tidyverse` until I'm ready to
start breaking the rules and flouting conventions for better results. However,
as a seasoned software engineer I will use my own judgement and preference for
writing readable and iconic code.

```{r packages}
install.packages("tidyverse")
install.packages("lubridate")
#install.packages("ggplot2")
#install.packages("ggmap")
#install.packages("maps")
#install.packages("viridis")
```

## Reading the data

Place the data within a folder called `data`. If downloading from the links
provided at the beginning of this document then file should already have the
correct name, else, rename it to `owid-covid-data.csv`. The data looks
pre-cleaned, which is nice. I may have to clean and up for certain activities
but I think it's better to do that just before visualization. If I end up
doing the same cleaning repeatedly I can just refactor. Refactoring is one of
the most useful skills a programmer can have and something greatly under used.

```{r reading-the-data}
library("tidyverse")

raw_data <- read_csv("./data/owid-covid-data.csv")
head(raw_data)
```

## Functions for extracting column options

Here I am just defining the functions that can be used to extract the distinct
values for particular columns, i.e. continents, countries, and etc.

```{r functions-for-extracting-column-options}

get_distinct_col <- function(data, col) {
  col_sym <- rlang::sym(col)
  
  r <- data %>%
    filter(!is.na(!!col_sym)) %>%
    select(!!col_sym) %>%
    distinct(!!col_sym) %>%
    arrange(!!col_sym)
  
  return (r)
}

get_geo_metadata <- function(data) {
  r <- data %>%
    group_by(continent, location, iso_code) %>%
    distinct(location) %>%
    summarise(
      continent,
      location,
      iso_code,
    ) %>%
    ungroup() %>%
    arrange(location)
  
  return (r)
}

get_all_continents <- function(data) {
  return (get_distinct_col(data, 'continent'))
}

get_all_locations <- function(data) {
  return (get_distinct_col(data, 'location'))
}

get_all_country_locations <- function(data) {
  r <- data %>%
    filter(!startsWith(iso_code, 'OWID_')) %>%
    select(location, iso_code) %>%
    arrange(location)
  
  return (r)
}

get_all_special_locations <- function(data) {
  r <- data %>%
    filter(startsWith(iso_code, 'OWID_')) %>%
    select(location, iso_code) %>%
    arrange(location)
  
  return (r)
}
```

## Listing column options

I've decided to pre-extract the column options so I can print them for
reference use them later. We could just call the extraction functions as and
when needed in this case but with much bigger data sets there might be a
execution speed hit. Then again, pre-extracting has a memory cost. I'm pretty
sure the selection of distinct values is pretty fast in any scenario.

```{r listing-column-options}
geo_metadata          <- get_geo_metadata(raw_data)
all_continents        <- get_all_continents(geo_metadata)
all_locations         <- get_all_locations(geo_metadata)
all_country_locations <- get_all_country_locations(geo_metadata)
all_special_locations <- get_all_special_locations(geo_metadata)

print(geo_metadata)
print(all_continents)
print(all_locations)
print(all_country_locations)
print(all_special_locations)
```

## Visualising total cases in European countries

To get started I'm going to create a simple visualisation showing the number
of total cases in a handful of European countries. Nothing original or fancy
but it will help to develop my understanding of the data.

```{r visualising-total-cases}
# TODO Do next.
```

## Functions for amalgamating columns by time period

```{r functions-for-amalgamating-columns-by-time-period}
library("lubridate")

# These columns use the double type but some are actually integers.
# Later, it might be worth creating a metadata file with:
# - column_name (string)
# - type (string)
# - nullable (bool)
# - summable (bool)
# - aggregatable (bool)
# I might as well create metadata files for locations whilst I'm at it

aggregatable_cols <- c(
  "total_cases",
  "new_cases",
  "new_cases_smoothed",
  "total_deaths",
  "new_deaths",
  "new_deaths_smoothed",
  "total_cases_per_million",
  "new_cases_per_million",
  "new_cases_smoothed_per_million",
  "total_deaths_per_million",
  "new_deaths_per_million",
  "new_deaths_smoothed_per_million",
  "reproduction_rate",
  "icu_patients",
  "icu_patients_per_million",
  "hosp_patients",
  "hosp_patients_per_million",
  "weekly_icu_admissions",
  "weekly_icu_admissions_per_million",
  "weekly_hosp_admissions",
  "weekly_hosp_admissions_per_million",
  "total_tests",
  "new_tests",
  "total_tests_per_thousand",
  "new_tests_per_thousand",
  "new_tests_smoothed",
  "new_tests_smoothed_per_thousand",
  "positive_rate",
  "tests_per_case",
  "total_vaccinations",
  "people_vaccinated",
  "people_fully_vaccinated",
  "total_boosters",
  "new_vaccinations",
  "new_vaccinations_smoothed",
  "total_vaccinations_per_hundred",
  "people_vaccinated_per_hundred",
  "people_fully_vaccinated_per_hundred",
  "total_boosters_per_hundred",
  "new_vaccinations_smoothed_per_million",
  "new_people_vaccinated_smoothed",
  "new_people_vaccinated_smoothed_per_hundred",
  "stringency_index",
  "population",
  "population_density",
  "median_age",
  "aged_65_older",
  "aged_70_older",
  "gdp_per_capita",
  "extreme_poverty",
  "cardiovasc_death_rate",
  "diabetes_prevalence",
  "female_smokers",
  "male_smokers",
  "handwashing_facilities",
  "hospital_beds_per_thousand",
  "life_expectancy",
  "human_development_index",
  "excess_mortality_cumulative_absolute",
  "excess_mortality_cumulative",
  "excess_mortality",
  "excess_mortality_cumulative_per_million"
)

# TODO Some columns need to be removed.
# TODO Including `total_`s as I think they are cumulative 'to date'  
summable_cols <- c(
  "total_cases",
  "new_cases",
  "new_cases_smoothed",
  "total_deaths",
  "new_deaths",
  "new_deaths_smoothed",
  "total_cases_per_million",
  "new_cases_per_million",
  "new_cases_smoothed_per_million",
  "total_deaths_per_million",
  "new_deaths_per_million",
  "new_deaths_smoothed_per_million",
  "reproduction_rate",
  "icu_patients",
  "icu_patients_per_million",
  "hosp_patients",
  "hosp_patients_per_million",
  "weekly_icu_admissions",
  "weekly_icu_admissions_per_million",
  "weekly_hosp_admissions",
  "weekly_hosp_admissions_per_million",
  "total_tests",
  "new_tests",
  "total_tests_per_thousand",
  "new_tests_per_thousand",
  "new_tests_smoothed",
  "new_tests_smoothed_per_thousand",
  "positive_rate",
  "tests_per_case",
  "total_vaccinations",
  "people_vaccinated",
  "people_fully_vaccinated",
  "total_boosters",
  "new_vaccinations",
  "new_vaccinations_smoothed",
  "total_vaccinations_per_hundred",
  "people_vaccinated_per_hundred",
  "people_fully_vaccinated_per_hundred",
  "total_boosters_per_hundred",
  "new_vaccinations_smoothed_per_million",
  "new_people_vaccinated_smoothed",
  "new_people_vaccinated_smoothed_per_hundred",
  "stringency_index",
  "population",
  "population_density",
  "median_age",
  "aged_65_older",
  "aged_70_older",
  "gdp_per_capita",
  "extreme_poverty",
  "cardiovasc_death_rate",
  "diabetes_prevalence",
  "female_smokers",
  "male_smokers",
  "handwashing_facilities",
  "hospital_beds_per_thousand",
  "life_expectancy",
  "human_development_index",
  "excess_mortality_cumulative_absolute",
  "excess_mortality_cumulative",
  "excess_mortality",
  "excess_mortality_cumulative_per_million"
)

yearly_sum <- function(data) {
  r <- data %>%
    mutate(year = year(date)) %>% 
    group_by(location, year) %>%
    summarise_at(summable_cols, sum, na.rm = TRUE) %>%
    ungroup() %>%
    arrange(location, year)
    
   return (r)
}

weekly_sum <- function(data) {
  r <- data %>%
    mutate(year = year(date)) %>% 
    mutate(week = week(date)) %>% 
    group_by(location, year, week) %>%
    summarise_at(summable_cols, sum, na.rm = TRUE) %>%
    ungroup() %>%
    arrange(location, year, week)
    
   return (r)
}

print(yearly_sum(raw_data))
print(weekly_sum(raw_data))
```

